knitr::opts_chunk$set(echo = TRUE)
head(un_speeches)
# Load the data
human_coding <- read_csv(files/'HumanCoding.csv')
# Load the data
human_coding <- read_csv(files/'HumanCoding.csv')
# Load the data
human_coding <- read_csv(files//'HumanCoding.csv')
# Load the data
human_coding <- read_csv('files//HumanCoding.csv')
# Load the data
human_coding <- read_csv('files/HumanCoding.csv')
un_speeches <- read_csv('file/un_speeches.csv')
library(readr)
library(dplyr)
# Load the data
human_coding <- read_csv('files/HumanCoding.csv')
un_speeches <- read_csv('file/un_speeches.csv')
un_speeches <- read_csv('files/un_speeches.csv')
head(human_coding)
head(un_speeches)
View(human_coding)
View(human_coding)
View(un_speeches)
View(un_speeches)
# Display the dataset
head(human_coding)
# Display the dataset
head(human_coding)
head(human_coding)
View(human_coding)
# Clean the 'text' column in the human_coding dataset
human_coding$clean_text <- sapply(human_coding$text, clean_text)
# install the necessary packages
install.packages("tm")
install.packages("SnowballC")
# Load the necessary libraries
library(tm)
library(SnowballC)
# Define a function to clean the text
clean_text <- function(text) {
# Convert to lowercase
text <- tolower(text)
# Remove punctuation
text <- removePunctuation(text)
# Remove numbers
text <- removeNumbers(text)
# Remove stopwords
text <- removeWords(text, stopwords('en'))
# Stem the words
text <- wordStem(text)
return(text)
}
# Clean the 'text' column in the human_coding dataset
human_coding$clean_text <- sapply(human_coding$text, clean_text)
# Display the dataset
head(human_coding)
install.packages("SnowballC")
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
# Load the data
human_coding <- read_csv('files/HumanCoding.csv')
un_speeches <- read_csv('files/un_speeches.csv')
# Display the first few rows of each dataset
head(human_coding)
# install the necessary packages
install.packages("tm")
install.packages("SnowballC")
# Load the necessary libraries
library(tm)
library(SnowballC)
# Define a function to clean the text
clean_text <- function(text) {
# Convert to lowercase
text <- tolower(text)
# Remove punctuation
text <- removePunctuation(text)
# Remove numbers
text <- removeNumbers(text)
# Remove stopwords
text <- removeWords(text, stopwords('en'))
# Stem the words
text <- wordStem(text)
return(text)
}
# Clean the 'text' column in the human_coding dataset
human_coding$clean_text <- sapply(human_coding$text, clean_text)
# Display the dataset
head(human_coding)
# Install and load the necessary library
install.packages('caTools')
library(caTools)
# Set the seed for reproducibility
set.seed(42)
# Perform the train-test split
split <- sample.split(human_coding$clean_text, SplitRatio = 0.8)
train_set <- subset(human_coding, split == TRUE)
test_set <- subset(human_coding, split == FALSE)
View(train_set)
View(train_set)
# Create a document-term matrix for the training set
dtm_train <- DocumentTermMatrix(Corpus(VectorSource(train_set$clean_text)))
# Create a document-term matrix for the test set
dtm_test <- DocumentTermMatrix(Corpus(VectorSource(test_set$clean_text)), control = list(dictionary = Terms(dtm_train)))
# Load the necessary library
library(e1071)
View(dtm_test)
View(dtm_test)
# Train the Naive Bayes model
nb_model <- naiveBayes(as.matrix(dtm_train), as.factor(train_set$coding))
# Train the Support Vector Machine model
svm_model <- svm(as.matrix(dtm_train), as.factor(train_set$coding), kernel = 'linear')
# install.packages("pacman")
pacman::p_load(tidyverse,
tidymodels,
naivebayes,
discrim)
# install.packages("pacman")
pacman::p_load(tidyverse,
tidymodels,
naivebayes,
discrim)
# read the files
un_speeches_sentences <- read.csv("files/HumanCoding.csv") %>%
dplyr::select(text)
#
text_rec <- recipes::recipe(coding ~ text, train_data) %>%
recipes::step_mutate(text = stringr::str_replace_all(text, "[[:digit:]]", "")) %>%
textrecipes::step_tokenize(text) %>%
textrecipes::step_stopwords(text, language = "english") %>%
textrecipes::step_stem(text, options = list(language = "english")) %>%
textrecipes::step_tokenfilter(text, max_tokens = 500) %>%
textrecipes::step_tfidf(text)
View(un_speeches_sentences)
View(un_speeches_sentences)
# install.packages("pacman")
pacman::p_load(tidyverse,
tidymodels,
naivebayes,
discrim)
# read the files
un_speeches_sentences <- read.csv("files/HumanCoding.csv") %>%
dplyr::select(text)
un_speeches_annotated <- read.csv("files/HumanCoding.csv")
set.seed(202305)
un_speeches_split <- initial_split(un_speeches_annotated, prop = 0.7, strata = coding)
train_data <- training(un_speeches_split)
test_data <- testing(un_speeches_split)
#Preprocessing the training dataset
text_rec <- recipes::recipe(coding ~ text, train_data) %>%
recipes::step_mutate(text = stringr::str_replace_all(text, "[[:digit:]]", "")) %>%
textrecipes::step_tokenize(text) %>%
textrecipes::step_stopwords(text, language = "english") %>%
textrecipes::step_stem(text, options = list(language = "english")) %>%
textrecipes::step_tokenfilter(text, max_tokens = 500) %>%
textrecipes::step_tfidf(text)
#Define the naive Bayes model
nb_mod <- parsnip::naive_Bayes(Laplace = 0.05, smoothness = 0.5) %>%
parsnip::set_mode("classification") %>%
parsnip::set_engine("naivebayes")
# Define Workflow
un_speeches_wflow <-
workflows::workflow() %>%
workflows::add_model(nb_mod) %>%
workflows::add_recipe(text_rec)
fitted_model <- parsnip::fit(un_speeches_wflow, data = train_data)
View(train_data)
fitted_model <- parsnip::fit(un_speeches_wflow, data = train_data)
predictions <- fitted_model %>%
predict(new_data = test_data) # (9)
test_data_pred <- test_data %>%
dplyr::bind_cols(predictions) %>%
dplyr::rename(pred = `.pred_class`) %>%
dplyr::mutate(coding = as.factor(coding))
conf_matrix <- test_data_pred %>% conf_mat(truth = coding, estimate = pred)
autoplot(conf_matrix, type = "heatmap")
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
install.packages("dplyr")
library(readr)
library(dplyr)
# Load the data
human_coding <- read_csv('files/HumanCoding.csv')
un_speeches <- read_csv('files/un_speeches.csv')
# Display the first few rows of each dataset
head(human_coding)
library(readr)
library(dplyr)
# Load the data
human_coding <- read_csv('files/HumanCoding.csv')
un_speeches <- read_csv('files/un_speeches.csv')
# Display the first few rows of each dataset
head(human_coding)
